import spacy
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# –ü–ê–†–ê–ú–ï–¢–†–ò –©–û –®–ï–ü–û–ß–£–¢–¨ –ú–ï–ù–Ü –í –¢–ï–ú–†–Ø–í–Ü
MAX_WORDS_PER_AUTHOR = 10000  # –í–û–ù–ò –°–ö–ê–ó–ê–õ–ò –ó–£–ü–ò–ù–ò–¢–ò–°–Ø –ù–ê 10000
TEXT_DIR = r"D:\Soft\PyCharm Projects\–ö–≤–∞–Ω—Ç–∏—Ç–∞—Ç–∏–≤–Ω–∞ –õ–∞–Ω–≥—É—Å—Ç–∏–∫–∞\–ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü—ñ—è\–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞ ‚Ññ4"  # –ú–Ü–°–¶–ï –î–ï –ñ–ò–í–£–¢–¨ –°–õ–û–í–ê
OUTPUT_DIR = "results"  # –ü–ê–ü–ö–ê-–°–•–û–í–ò–©–ï –î–õ–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í
os.makedirs(OUTPUT_DIR, exist_ok=True)  # –°–¢–í–û–†–Æ–Ñ–ú–û –°–•–û–í–ò–©–ï –Ø–ö–©–û –ô–û–ì–û –ù–ï–ú–ê–Ñ

# –°–õ–û–í–ê-–ü–†–ò–ú–ê–†–ò –Ø–ö–Ü –¢–†–ï–ë–ê –í–ò–ì–ù–ê–¢–ò
STOP_WORDS = {'–±—É—Ç–∏', '–º–∞—Ç–∏', '—Ä–æ–±–∏—Ç–∏', '—Å—Ç–∞–≤–∞—Ç–∏', '–≥–æ–≤–æ—Ä–∏—Ç–∏', '—Ç–æ–π', '—Ü–µ–π', '—Ç–∞–∫–∏–π', '—è–∫–∏–π', '–≤–µ—Å—å'}

# –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–á –î–õ–Ø –û–ß–ï–ô –Ø–ö–Ü –ë–ê–ß–ê–¢–¨ –ó–ê–ë–ê–ì–ê–¢–û
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

nlp = spacy.load("uk_core_news_lg")  # –ó–í–Ü–† –ü–†–û–ö–ò–ù–£–í–°–Ø


def extract_phrases(doc):
    """–í–ò–¢–Ø–ì–£–Ñ–ú–û –°–õ–û–í–û–°–ü–û–õ–£–ß–ï–ù–ù–Ø –ó –¢–ï–ú–†–Ø–í–ò (–≤–æ–Ω–∏ —à–µ–ø–æ—á—É—Ç—å—Å—è –º—ñ–∂ —Å–æ–±–æ—é)"""
    phrases = {'verbal': [], 'nominal': [], 'adverbial': []}

    for token in doc:
        # –î–Ü–Ñ–°–õ–û–í–ê –®–£–ö–ê–Æ–¢–¨ –î–†–£–ó–Ü–í (—ó–º –ø–æ—Ç—Ä—ñ–±–Ω—ñ —ñ–º–µ–Ω–Ω–∏–∫–∏ —Ç–∞ –ø—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–∏)
        if token.pos_ == "VERB":
            for child in token.children:
                # –î–Ü–Ñ–°–õ–û–í–û + –Ü–ú–ï–ù–ù–ò–ö (–≤–æ–Ω–∏ —Ç—Ä–∏–º–∞—é—Ç—å—Å—è –∑–∞ —Ä—É–∫–∏ –≤ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–Ω–æ–º—É –¥–µ—Ä–µ–≤—ñ)
                if child.pos_ == "NOUN" and child.dep_ in ["obj", "iobj", "obl", "nsubj", "nmod"]:
                    phrases['verbal'].append(f"{token.lemma_} {child.lemma_}")
                # –î–Ü–Ñ–°–õ–û–í–û + –ü–†–ò–°–õ–Ü–í–ù–ò–ö (—Ç–∏—Ö–∏–π —à–µ–ø—ñ—Ç —É —Ç–µ–º—Ä—è–≤—ñ)
                elif child.pos_ == "ADV" and child.dep_ == "advmod":
                    phrases['verbal'].append(f"{token.lemma_} {child.lemma_}")

            # –Ø–ö–©–û –î–Ü–Ñ–°–õ–û–í–û –°–õ–£–•–ê–Ñ –Ü–ú–ï–ù–ù–ò–ö (—ñ–Ω–≤–µ—Ä—Å—ñ—è –≤–ª–∞–¥–∏)
            if token.head.pos_ == "NOUN":
                phrases['verbal'].append(f"{token.lemma_} {token.head.lemma_}")

        # –Ü–ú–ï–ù–ù–ò–ö–ò –¢–ê–ù–¶–Æ–Æ–¢–¨ –ó –ü–†–ò–ö–ú–ï–¢–ù–ò–ö–ê–ú–ò (–Ω–æ–º—ñ–Ω–∞–ª—å–Ω–∏–π –±–∞–ª)
        if token.pos_ == "NOUN":
            for child in token.children:
                # –ü–†–ò–ö–ú–ï–¢–ù–ò–ö + –Ü–ú–ï–ù–ù–ò–ö (–∫–ª–∞—Å–∏—á–Ω–∞ –ø–∞—Ä–∞)
                if child.pos_ == "ADJ":
                    phrases['nominal'].append(f"{child.lemma_} {token.lemma_}")
                # –Ü–ú–ï–ù–ù–ò–ö + –Ü–ú–ï–ù–ù–ò–ö (–∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏–π —Å–æ—é–∑)
                elif child.pos_ == "NOUN":
                    phrases['nominal'].append(f"{token.lemma_} {child.lemma_}")

            # –ó–í–û–†–û–¢–ù–ò–ô –ó–í'–Ø–ó–û–ö (—è–∫—â–æ —ñ–º–µ–Ω–Ω–∏–∫ –ø—ñ–¥–ø–æ—Ä—è–¥–∫–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏–∫–º–µ—Ç–Ω–∏–∫—É)
            if token.head.pos_ == "ADJ":
                phrases['nominal'].append(f"{token.head.lemma_} {token.lemma_}")

        # –ü–†–ò–°–õ–Ü–í–ù–ò–ö–ò –•–û–í–ê–Æ–¢–¨–°–Ø –í –¢–Ü–ù–Ü (–º–æ–¥–∏—Ñ—ñ–∫—É—é—Ç—å —É—Å—ñ—Ö –ø—ñ–¥—Ä—è–¥)
        if token.pos_ == "ADV":
            # –ü–†–ò–°–õ–Ü–í–ù–ò–ö –®–ï–ü–û–ß–ï –î–Ü–Ñ–°–õ–û–í–£
            if token.head.pos_ == "VERB":
                phrases['adverbial'].append(f"{token.lemma_} {token.head.lemma_}")
            # –ü–†–ò–°–õ–Ü–í–ù–ò–ö –¢–û–†–ö–ê–Ñ–¢–¨–°–Ø –ü–†–ò–ö–ú–ï–¢–ù–ò–ö–ê
            elif token.head.pos_ == "ADJ":
                phrases['adverbial'].append(f"{token.lemma_} {token.head.lemma_}")
            # –ü–†–ò–°–õ–Ü–í–ù–ò–ö –†–û–ó–ú–û–í–õ–Ø–Ñ –ó –Ü–ù–®–ò–ú –ü–†–ò–°–õ–Ü–í–ù–ò–ö–û–ú (–ª—É–Ω–∞ –≤ –ø–æ—Ä–æ–∂–Ω–µ—á—ñ)
            elif token.head.pos_ == "ADV":
                phrases['adverbial'].append(f"{token.lemma_} {token.head.lemma_}")

    return phrases


def filter_phrases(phrases_dict):
    """–§–Ü–õ–¨–¢–†–£–Ñ–ú–û –®–ï–ü–Ü–¢ –í–Ü–î –ë–†–ï–•–õ–ò–í–ò–• –°–õ–Ü–í (—Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ç–∞ –ø–æ–≤—Ç–æ—Ä–∏)"""
    filtered = {'verbal': [], 'nominal': [], 'adverbial': []}

    for phrase_type, phrase_list in phrases_dict.items():
        for phrase in phrase_list:
            words = phrase.split()
            # –ü–ï–†–ï–í–Ü–†–Ø–Ñ–ú–û –ù–ê –ü–†–ò–ú–ê–†–Ü–í –Ü –ö–ê–†–õ–ò–ö–Ü–í
            if len(words) == 2:
                # –í–ò–ì–ù–ê–ù–ù–Ø –ü–û–í–¢–û–†–Ü–í (–∑–∞—ó–∫–∞—é—á—ñ—Å—è —Å–ª–æ–≤–∞)
                if words[0] == words[1]:
                    continue
                # –í–ò–ì–ù–ê–ù–ù–Ø –°–¢–û–ü-–°–õ–Ü–í –Ü –ó–ê–ö–û–†–û–¢–ö–ò–• (–≤–æ–Ω–∏ —Å–ø–æ—Å—Ç–µ—Ä—ñ–≥–∞—é—Ç—å –∑–∞ –Ω–∞–º–∏)
                if (words[0] not in STOP_WORDS and words[1] not in STOP_WORDS and
                        len(words[0]) > 2 and len(words[1]) > 2):
                    filtered[phrase_type].append(phrase)

    return filtered


# === –ß–ò–¢–ê–ù–ù–Ø –¢–ê –û–ë–†–û–ë–ö–ê –¢–ï–ö–°–¢–Ü–í (–≤–æ–Ω–∏ –ø—Ä–∏—Ö–æ–¥—è—Ç—å —ñ–∑ —Ç–µ–º—Ä—è–≤–∏) ===
print("üìö –ó–ê–í–ê–ù–¢–ê–ñ–£–Ñ–ú–û –¢–ï–ö–°–¢–ò... (—Å–ª–æ–≤–∞ –≤–∏—à–∏–∫–æ–≤—É—é—Ç—å—Å—è –≤ —Ä—è–¥–∏)")
all_data = []
author_stats = {}

for file in os.listdir(TEXT_DIR):
    if file.endswith(".txt"):  # –¢–ï–ö–°–¢–û–í–Ü –§–ê–ô–õ–ò - –¶–ï –ü–û–†–¢–ê–õ–ò
        author = os.path.splitext(file)[0]
        print(f"  –û–ë–†–û–ë–õ–Ø–Ñ–ú–û: {author} (—â–µ –æ–¥–Ω–∞ –∂–µ—Ä—Ç–≤–∞)")

        with open(os.path.join(TEXT_DIR, file), encoding="utf-8") as f:
            words = f.read().split()[:MAX_WORDS_PER_AUTHOR]  # –ü–Ü–î–†–Ü–ó–ê–Ñ–ú–û –ö–†–ò–õ–ê –°–õ–Ü–í–ê–ú
            text = " ".join(words)  # –ó'–Ñ–î–ù–£–Ñ–ú–û –á–• –£ –õ–ê–ù–¶–Æ–ñ–û–ö

        doc = nlp(text)  # –ü–†–û–ü–£–°–ö–ê–Ñ–ú–û –ö–†–Ü–ó–¨ –ú–õ–ò–ù
        phrases = extract_phrases(doc)  # –í–ò–¢–Ø–ì–£–Ñ–ú–û –°–ü–û–õ–£–ß–ï–ù–ù–Ø
        phrases = filter_phrases(phrases)  # –§–Ü–õ–¨–¢–†–£–Ñ–ú–û –®–£–ú

        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–õ–Ø –ë–û–ì–Ü–í –î–ê–ù–ò–•
        author_stats[author] = {
            'words': len(words),  # –°–ö–Ü–õ–¨–ö–ò –°–õ–Ü–í –ü–û–ë–ê–ß–ò–í
            'verbal': len(phrases['verbal']),  # –î–Ü–Ñ–°–õ–Ü–í–ù–Ü –®–ï–ü–û–¢–ò
            'nominal': len(phrases['nominal']),  # –Ü–ú–ï–ù–ù–Ü –¢–Ü–ù–Ü
            'adverbial': len(phrases['adverbial']),  # –ü–†–ò–°–õ–Ü–í–ù–ò–ö–û–í–Ü –õ–£–ù–ò
            'total': sum(len(p) for p in phrases.values())  # –í–°–ï –†–ê–ó–û–ú
        }

        for typ, items in phrases.items():
            for phrase in items:
                all_data.append({"author": author, "type": typ, "phrase": phrase})

df = pd.DataFrame(all_data)  # –ü–ï–†–ï–¢–í–û–†–Æ–Ñ–ú–û –•–ê–û–° –ù–ê –¢–ê–ë–õ–ò–¶–Æ
print(f"\n‚úÖ –ó–ù–ê–ô–î–ï–ù–û {len(df)} –°–õ–û–í–û–°–ü–û–õ–£–ß–ï–ù–¨ –í–Ü–î {len(author_stats)} –ê–í–¢–û–†–Ü–í (–≤–æ–Ω–∏ —Å–µ—Ä–µ–¥ –Ω–∞—Å)")

# –í–ò–í–û–î–ò–ú–û –°–¢–ê–¢–ò–°–¢–ò–ö–£ (—Ü–∏—Ñ—Ä–∏ —à–µ–ø–æ—á—É—Ç—å –ø—Ä–∞–≤–¥—É)
print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ê–í–¢–û–†–ê–•:")
stats_df = pd.DataFrame(author_stats).T
print(stats_df)
stats_df.to_excel(f"{OUTPUT_DIR}/statistics.xlsx")  # –ó–ë–ï–†–Ü–ì–ê–Ñ–ú–û –î–õ–Ø –ú–ê–ô–ë–£–¢–ù–Ü–• –ü–û–ö–û–õ–Ü–ù–¨

# === –ß–ê–°–¢–û–¢–ù–ò–ô –ê–ù–ê–õ–Ü–ó (—Ä–∞—Ö—É—î–º–æ —à–µ–ø—ñ—Ç) ===
print("\nüî¢ –ß–ê–°–¢–û–¢–ù–ò–ô –ê–ù–ê–õ–Ü–ó... (—Å–ª–æ–≤–∞ –º–µ—Ä–µ—Ö—Ç—è—Ç—å –∑ —Ä—ñ–∑–Ω–æ—é —á–∞—Å—Ç–æ—Ç–æ—é)")
freq_df = (
    df.groupby(["author", "type", "phrase"])
    .size()
    .reset_index(name="count")
    .sort_values(["author", "type", "count"], ascending=[True, True, False])
)

# –ó–ë–ï–†–Ü–ì–ê–Ñ–ú–û –í EXCEL (–∫–ª–∞–¥–µ–º–æ –≤ —Ä—ñ–∑–Ω—ñ –∫—ñ–º–Ω–∞—Ç–∏)
with pd.ExcelWriter(f"{OUTPUT_DIR}/phrases_by_type.xlsx", engine="openpyxl") as writer:
    for t in ["verbal", "nominal", "adverbial"]:
        freq_df[freq_df["type"] == t].to_excel(writer, sheet_name=t, index=False)

# === –ê–ù–ê–õ–Ü–ó –£–ù–Ü–ö–ê–õ–¨–ù–û–°–¢–Ü (—à—É–∫–∞—î–º–æ –≤—ñ–¥–±–∏—Ç–∫–∏ –ø–∞–ª—å—Ü—ñ–≤) ===
print("\nüîç –ê–ù–ê–õ–Ü–ó –£–ù–Ü–ö–ê–õ–¨–ù–û–°–¢–Ü... (—É –∫–æ–∂–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–∞ —Å–≤—ñ–π –ø–æ—á–µ—Ä–∫)")
authors = df['author'].unique()
uniqueness_data = []

for phrase_type in ["verbal", "nominal", "adverbial"]:
    type_df = freq_df[freq_df["type"] == phrase_type]

    for author in authors:
        author_phrases = set(type_df[type_df["author"] == author]["phrase"])  # –°–õ–û–í–ê –ê–í–¢–û–†–ê
        other_phrases = set(type_df[type_df["author"] != author]["phrase"])   # –°–õ–û–í–ê –Ü–ù–®–ò–•

        unique = len(author_phrases - other_phrases)    # –£–ù–Ü–ö–ê–õ–¨–ù–Ü –®–ï–ü–û–¢–ò
        shared = len(author_phrases & other_phrases)    # –°–ü–Ü–õ–¨–ù–Ü –ì–û–õ–û–°–ò
        total = len(author_phrases)                     # –í–°–ï –†–ê–ó–û–ú

        uniqueness_data.append({
            "author": author,
            "type": phrase_type,
            "unique": unique,
            "shared": shared,
            "total": total,
            "unique_pct": (unique / total * 100) if total > 0 else 0  # –í–Ü–î–°–û–¢–û–ö –£–ù–Ü–ö–ê–õ–¨–ù–û–°–¢–Ü
        })

uniqueness_df = pd.DataFrame(uniqueness_data)

# –ì–†–ê–§–Ü–ö –£–ù–Ü–ö–ê–õ–¨–ù–û–°–¢–Ü (–≤—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ —Ä–æ–∑–¥–≤–æ—î–Ω–Ω—è –æ—Å–æ–±–∏—Å—Ç–æ—Å—Ç—ñ)
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
types = ["verbal", "nominal", "adverbial"]
type_labels = ["–î–Ü–Ñ–°–õ–Ü–í–ù–Ü", "–Ü–ú–ï–ù–ù–Ü", "–ü–†–ò–°–õ–Ü–í–ù–ò–ö–û–í–Ü"]

for idx, (t, label) in enumerate(zip(types, type_labels)):
    data = uniqueness_df[uniqueness_df["type"] == t]
    x = np.arange(len(data))
    width = 0.35

    axes[idx].bar(x - width / 2, data["unique"], width, label='–£–ù–Ü–ö–ê–õ–¨–ù–Ü', alpha=0.8)
    axes[idx].bar(x + width / 2, data["shared"], width, label='–°–ü–Ü–õ–¨–ù–Ü', alpha=0.8)

    axes[idx].set_xlabel('–ê–í–¢–û–†')
    axes[idx].set_ylabel('–ö–Ü–õ–¨–ö–Ü–°–¢–¨')
    axes[idx].set_title(f'{label} –°–õ–û–í–û–°–ü–û–õ–£–ß–ï–ù–ù–Ø (—à–µ–ø—ñ—Ç —ñ –ª—É–Ω–∞)')
    axes[idx].set_xticks(x)
    axes[idx].set_xticklabels([a.split('_')[0] for a in data["author"]], rotation=45, ha='right')
    axes[idx].legend()
    axes[idx].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/uniqueness_analysis.png", bbox_inches='tight')
plt.close()

# === –¢–û–ü-10 –°–õ–û–í–û–°–ü–û–õ–£–ß–ï–ù–¨ (–Ω–∞–π–≥—É—á–Ω—ñ—à—ñ —à–µ–ø–æ—Ç–∏) ===
print("\nüìä –°–¢–í–û–†–Æ–Ñ–ú–û –ì–†–ê–§–Ü–ö–ò –¢–û–ü-10... (–≥–æ–ª–æ—Å–∏ —Å—Ç–∞—é—Ç—å –≥—É—á–Ω—ñ—à–∏–º–∏)")
for t, label in zip(types, type_labels):
    fig, axes = plt.subplots(1, len(authors), figsize=(12, 6), sharey=True)
    if len(authors) == 1:
        axes = [axes]

    for idx, author in enumerate(authors):
        top10 = (
            freq_df[(freq_df["type"] == t) & (freq_df["author"] == author)]
            .nlargest(10, "count")
            .sort_values("count", ascending=True)
        )

        colors = sns.color_palette("viridis", len(top10))
        axes[idx].barh(range(len(top10)), top10["count"], color=colors)
        axes[idx].set_yticks(range(len(top10)))
        axes[idx].set_yticklabels(top10["phrase"], fontsize=9)
        axes[idx].set_xlabel('–ß–ê–°–¢–û–¢–ê')
        axes[idx].set_title(author.split('_')[0], fontsize=10, fontweight='bold')
        axes[idx].grid(axis='x', alpha=0.3)

        # –î–û–î–ê–Ñ–ú–û –¶–ò–§–†–ò (–≤–æ–Ω–∏ –∫—Ä–∏—á–∞—Ç—å –∑ –≤–µ—Ä—à–∏–Ω)
        for i, v in enumerate(top10["count"]):
            axes[idx].text(v, i, f' {v}', va='center', fontsize=8)

    axes[0].set_ylabel('–°–õ–û–í–û–°–ü–û–õ–£–ß–ï–ù–ù–Ø')
    fig.suptitle(f'–¢–û–ü-10 {label} –®–ï–ü–û–¢–Ü–í', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/top10_{t}.png", bbox_inches='tight')
    plt.close()

# === –í–ï–ö–¢–û–†–ò–ó–ê–¶–Ü–Ø (–ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —Å–ª–æ–≤–∞ –Ω–∞ —á–∏—Å–ª–∞) ===
print("\nüßÆ –í–ï–ö–¢–û–†–ù–ò–ô –ê–ù–ê–õ–Ü–ó... (—Å–ª–æ–≤–∞ —Å—Ç–∞—é—Ç—å —Ç–æ—á–∫–∞–º–∏ —É –ø—Ä–æ—Å—Ç–æ—Ä—ñ)")
sentences = [p.split() for p in df["phrase"]]
model = Word2Vec(sentences, vector_size=100, window=3, min_count=1, workers=4, epochs=20)  # –ù–ê–í–ß–ê–Ñ–ú–û –ú–û–ó–û–ö


def phrase_vector(phrase):
    """–ü–ï–†–ï–¢–í–û–†–Æ–Ñ–ú–û –§–†–ê–ó–£ –ù–ê –í–ï–ö–¢–û–† (—Å–ª–æ–≤–æ —Å—Ç–∞—î —á–∏—Å–ª–æ–º, —á–∏—Å–ª–æ —Å—Ç–∞—î –¥–æ–ª–µ—é)"""
    words = phrase.split()
    vectors = [model.wv[w] for w in words if w in model.wv]
    return np.mean(vectors, axis=0) if vectors else None


df["vector"] = df["phrase"].apply(phrase_vector)
df = df.dropna(subset=["vector"])  # –í–ò–î–ê–õ–Ø–Ñ–ú–û –ü–£–°–¢–û–¢–£

# === PCA –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø (–ø—Ä–æ–µ–∫—Ü—ñ—è –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–æ–≥–æ –∫–æ—à–º–∞—Ä—É) ===
print("\nüìâ PCA –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø... (—Å—Ç–∏—Å–∫–∞—î–º–æ –≤–∏–º—ñ—Ä–∏)")
agg_df = (
    df.groupby(["author", "type"])["vector"]
    .apply(lambda v: np.mean(np.vstack(v), axis=0))
    .reset_index()
)

pca = PCA(n_components=2)  # –ó–í–û–î–ò–ú–û –î–û 2-–• –í–ò–ú–Ü–†–Ü–í
coords = pca.fit_transform(list(agg_df["vector"]))
agg_df["x"], agg_df["y"] = coords[:, 0], coords[:, 1]

# –ì–†–ê–§–Ü–ö PCA (–∫–∞—Ä—Ç–∞ –∑–æ—Ä—è–Ω–æ–≥–æ –Ω–µ–±–∞ –∑—ñ —Å–ª—ñ–≤)
plt.figure(figsize=(10, 7))
markers = {'verbal': 'o', 'nominal': 's', 'adverbial': '^'}
colors = sns.color_palette("Set2", len(authors))

for idx, author in enumerate(authors):
    author_data = agg_df[agg_df["author"] == author]
    for ptype in ['verbal', 'nominal', 'adverbial']:
        type_data = author_data[author_data["type"] == ptype]
        if not type_data.empty:
            plt.scatter(
                type_data["x"], type_data["y"],
                marker=markers[ptype],
                s=300,
                color=colors[idx],
                label=f"{author.split('_')[0]} - {ptype}",
                edgecolors='black',
                linewidths=1.5,
                alpha=0.7
            )

            # –ü–Ü–î–ü–ò–°–ò (–º—ñ—Ç–∫–∏ –Ω–∞ –∫–∞—Ä—Ç—ñ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ)
            for _, row in type_data.iterrows():
                plt.annotate(
                    ptype[:3],
                    (row["x"], row["y"]),
                    fontsize=8,
                    ha='center',
                    va='center',
                    fontweight='bold'
                )

plt.xlabel(f'PCA 1 ({pca.explained_variance_ratio_[0]:.1%} –î–ò–°–ü–ï–†–°–Ü–á)', fontsize=12)
plt.ylabel(f'PCA 2 ({pca.explained_variance_ratio_[1]:.1%} –î–ò–°–ü–ï–†–°–Ü–á)', fontsize=12)
plt.title('–í–ï–ö–¢–û–†–ù–Ü –í–Ü–î–°–¢–ê–ù–Ü –ú–Ü–ñ –¢–ò–ü–ê–ú–ò –°–õ–Ü–í', fontsize=14, fontweight='bold')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/pca_analysis.png", bbox_inches='tight')
plt.close()

# === –¢–ï–ü–õ–û–í–ê –ö–ê–†–¢–ê –°–•–û–ñ–û–°–¢–Ü (–º–∞—Ç—Ä–∏—Ü—è –∑–≤'—è–∑–∫—ñ–≤) ===
print("\nüî• –¢–ï–ü–õ–û–í–ê –ö–ê–†–¢–ê –°–•–û–ñ–û–°–¢–Ü... (–≤–∏–º—ñ—Ä—é—î–º–æ —Ä–æ–¥–∏–Ω–Ω—ñ—Å—Ç—å –¥—É—à)")
vectors_matrix = np.vstack(agg_df["vector"].values)
similarity = cosine_similarity(vectors_matrix)  # –ö–û–°–ò–ù–£–°–ù–ê –†–û–î–ò–ù–ù–Ü–°–¢–¨

labels = [f"{row['author'].split('_')[0]}\n{row['type'][:3]}"
          for _, row in agg_df.iterrows()]

plt.figure(figsize=(10, 8))
sns.heatmap(
    similarity,
    annot=True,
    fmt='.2f',
    cmap='RdYlGn',
    xticklabels=labels,
    yticklabels=labels,
    cbar_kws={'label': '–ö–û–°–ò–ù–£–°–ù–ê –°–•–û–ñ–Ü–°–¢–¨'},
    square=True,
    linewidths=0.5
)
plt.title('–°–•–û–ñ–Ü–°–¢–¨ –í–ï–ö–¢–û–†–ù–ò–• –ü–†–ï–î–°–¢–ê–í–õ–ï–ù–¨', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/similarity_heatmap.png", bbox_inches='tight')
plt.close()

# === –†–û–ó–ü–û–î–Ü–õ –ß–ê–°–¢–û–¢ (—á–∞—Å—Ç–æ—Ç–Ω–∏–π —Å–ø–µ–∫—Ç—Ä) ===
print("\nüìà –†–û–ó–ü–û–î–Ü–õ –ß–ê–°–¢–û–¢... (—Ä–∏—Ç–º–∏ —Ç–∞ –ø–∞—Ç–µ—Ä–Ω–∏)")
fig, axes = plt.subplots(len(authors), 3, figsize=(15, 5 * len(authors)))
if len(authors) == 1:
    axes = axes.reshape(1, -1)

for row_idx, author in enumerate(authors):
    for col_idx, (t, label) in enumerate(zip(types, type_labels)):
        ax = axes[row_idx, col_idx]

        type_freq = freq_df[(freq_df["author"] == author) & (freq_df["type"] == t)]
        freq_counts = Counter(type_freq["count"])

        x = sorted(freq_counts.keys())
        y = [freq_counts[k] for k in x]

        ax.bar(x, y, color=sns.color_palette("muted")[col_idx], alpha=0.7, edgecolor='black')
        ax.set_xlabel('–ß–ê–°–¢–û–¢–ê –°–õ–û–í–û–°–ü–û–õ–£–ß–ï–ù–ù–Ø')
        ax.set_ylabel('–ö–Ü–õ–¨–ö–Ü–°–¢–¨ –£–ù–Ü–ö–ê–õ–¨–ù–ò–• –§–†–ê–ó')
        ax.set_title(f'{author.split("_")[0]} - {label}', fontweight='bold')
        ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/frequency_distribution.png", bbox_inches='tight')
plt.close()

# === –î–û–î–ê–¢–ö–û–í–ò–ô –ê–ù–ê–õ–Ü–ó (–≥–ª–∏–±–∏–Ω–∏ –±–æ–∂–µ–≤—ñ–ª–ª—è) ===
print("\nüìä –î–û–î–ê–¢–ö–û–í–ò–ô –°–¢–ê–¢–ò–°–¢–ò–ß–ù–ò–ô –ê–ù–ê–õ–Ü–ó... (–∫–æ–ø–∞—î–º–æ –≥–ª–∏–±—à–µ)")

# –°–ï–†–ï–î–ù–Ü –ß–ê–°–¢–û–¢–ò (–±–∞–ª–∞–Ω—Å —Å–∏–ª)
avg_freq_data = []
for author in authors:
    for ptype in types:
        type_freq = freq_df[(freq_df["author"] == author) & (freq_df["type"] == ptype)]
        if len(type_freq) > 0:
            avg_freq_data.append({
                "author": author,
                "type": ptype,
                "mean_freq": type_freq["count"].mean(),
                "median_freq": type_freq["count"].median(),
                "max_freq": type_freq["count"].max(),
                "unique_phrases": len(type_freq)
            })

avg_freq_df = pd.DataFrame(avg_freq_data)

# –ì–†–ê–§–Ü–ö –°–ï–†–ï–î–ù–Ü–• –ß–ê–°–¢–û–¢
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
type_labels = ["–î–Ü–Ñ–°–õ–Ü–í–ù–Ü", "–Ü–ú–ï–ù–ù–Ü", "–ü–†–ò–°–õ–Ü–í–ù–ò–ö–û–í–Ü"]

for idx, (t, label) in enumerate(zip(types, type_labels)):
    data = avg_freq_df[avg_freq_df["type"] == t]
    x = np.arange(len(data))
    width = 0.25

    axes[idx].bar(x - width, data["mean_freq"], width, label='–°–ï–†–ï–î–ù–Ø', alpha=0.8)
    axes[idx].bar(x, data["median_freq"], width, label='–ú–ï–î–Ü–ê–ù–ê', alpha=0.8)
    axes[idx].bar(x + width, data["max_freq"], width, label='–ú–ê–ö–°–ò–ú–£–ú', alpha=0.8)

    axes[idx].set_xlabel('–ê–í–¢–û–†')
    axes[idx].set_ylabel('–ß–ê–°–¢–û–¢–ê')
    axes[idx].set_title(f'{label} (—Ç—Ä–∏ –æ–±–ª–∏—á—á—è —á–∞—Å—Ç–æ—Ç–∏)')
    axes[idx].set_xticks(x)
    axes[idx].set_xticklabels([a.split('_')[0] for a in data["author"]], rotation=45, ha='right')
    axes[idx].legend()
    axes[idx].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/average_frequencies.png", bbox_inches='tight')
plt.close()

# –Ü–ù–î–ï–ö–° –ñ–ê–ö–ö–ê–†–ê (–≤–∏–º—ñ—Ä—é—î–º–æ –ø–µ—Ä–µ—Ç–∏–Ω —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–µ–π)
print("\nüîç –†–û–ó–†–ê–•–£–ù–û–ö –Ü–ù–î–ï–ö–°–£ –ñ–ê–ö–ö–ê–†–ê... (—Å–ø—ñ–ª—å–Ω—ñ —Å–Ω–∏)")
jaccard_data = []

for ptype in types:
    if len(authors) >= 2:
        author1_phrases = set(freq_df[(freq_df["author"] == authors[0]) & (freq_df["type"] == ptype)]["phrase"])
        author2_phrases = set(freq_df[(freq_df["author"] == authors[1]) & (freq_df["type"] == ptype)]["phrase"])

        intersection = len(author1_phrases & author2_phrases)  # –ü–ï–†–ï–¢–ò–ù –°–í–Ü–¢–Ü–í
        union = len(author1_phrases | author2_phrases)        # –û–ë'–Ñ–î–ù–ê–ù–ê –†–ï–ê–õ–¨–ù–Ü–°–¢–¨
        jaccard = intersection / union if union > 0 else 0    # –ö–û–ï–§–Ü–¶–Ü–Ñ–ù–¢ –†–û–î–ò–ù–ù–û–°–¢–Ü

        jaccard_data.append({
            "type": ptype,
            "author1_unique": len(author1_phrases - author2_phrases),  # –£–ù–Ü–ö–ê–õ–¨–ù–Ü –°–ù–ò
            "author2_unique": len(author2_phrases - author1_phrases),
            "shared": intersection,                                    # –°–ü–Ü–õ–¨–ù–Ü –ë–ê–ß–ï–ù–ù–Ø
            "jaccard_index": jaccard,
            "overlap_pct": (intersection / min(len(author1_phrases), len(author2_phrases)) * 100) if min(
                len(author1_phrases), len(author2_phrases)) > 0 else 0  # –í–Ü–î–°–û–¢–û–ö –ü–ï–†–ï–ö–†–ò–¢–¢–Ø
        })

jaccard_df = pd.DataFrame(jaccard_data)

# –ì–†–ê–§–Ü–ö –Ü–ù–î–ï–ö–°–£ –ñ–ê–ö–ö–ê–†–ê
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# –°–¢–û–í–ü–ß–ò–ö–ò –Ü–ù–î–ï–ö–°–£ –ñ–ê–ö–ö–ê–†–ê
x = np.arange(len(jaccard_df))
colors = sns.color_palette("viridis", len(jaccard_df))
bars = ax1.bar(x, jaccard_df["jaccard_index"], color=colors, alpha=0.7, edgecolor='black')
ax1.set_xlabel('–¢–ò–ü –°–õ–û–í–û–°–ü–û–õ–£–ß–ï–ù–¨')
ax1.set_ylabel('–Ü–ù–î–ï–ö–° –ñ–ê–ö–ö–ê–†–ê')
ax1.set_title('–°–•–û–ñ–Ü–°–¢–¨ –°–õ–û–í–ù–ò–ö–Ü–í –ú–Ü–ñ –ê–í–¢–û–†–ê–ú–ò\n(–Ü–ù–î–ï–ö–° –°–ü–Ü–õ–¨–ù–ò–• –°–ù–Ü–í)', fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(['–î–Ü–Ñ–°–õ–Ü–í–ù–Ü', '–Ü–ú–ï–ù–ù–Ü', '–ü–†–ò–°–õ–Ü–í–ù–ò–ö–û–í–Ü'])
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)

# –î–û–î–ê–Ñ–ú–û –ó–ù–ê–ß–ï–ù–ù–Ø (—Ü–∏—Ñ—Ä–∏ –Ω–∞ —Å—Ç–æ–≤–ø—á–∏–∫–∞—Ö)
for i, (bar, val) in enumerate(zip(bars, jaccard_df["jaccard_index"])):
    ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,
             f'{val:.3f}', ha='center', va='bottom', fontweight='bold')

# –°–¢–û–í–ü–ß–ò–ö–ò –£–ù–Ü–ö–ê–õ–¨–ù–ò–• VS –°–ü–Ü–õ–¨–ù–ò–•
type_names = ['–î–Ü–Ñ–°–õ–Ü–í–ù–Ü', '–Ü–ú–ï–ù–ù–Ü', '–ü–†–ò–°–õ–Ü–í–ù–ò–ö–û–í–Ü']
x2 = np.arange(len(jaccard_df))
width = 0.25

bars1 = ax2.bar(x2 - width, jaccard_df["author1_unique"], width,
                label=authors[0].split('_')[0], alpha=0.8)
bars2 = ax2.bar(x2, jaccard_df["shared"], width,
                label='–°–ü–Ü–õ–¨–ù–Ü', alpha=0.8)
bars3 = ax2.bar(x2 + width, jaccard_df["author2_unique"], width,
                label=authors[1].split('_')[0], alpha=0.8)

ax2.set_xlabel('–¢–ò–ü –°–õ–û–í–û–°–ü–û–õ–£–ß–ï–ù–¨')
ax2.set_ylabel('–ö–Ü–õ–¨–ö–Ü–°–¢–¨ –§–†–ê–ó')
ax2.set_title('–†–û–ó–ü–û–î–Ü–õ –£–ù–Ü–ö–ê–õ–¨–ù–ò–• –¢–ê –°–ü–Ü–õ–¨–ù–ò–• –§–†–ê–ó', fontweight='bold')
ax2.set_xticks(x2)
ax2.set_xticklabels(type_names)
ax2.legend()
ax2.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/jaccard_analysis.png", bbox_inches='tight')
plt.close()

# –ó–ë–ï–†–Ü–ì–ê–Ñ–ú–û –°–¢–ê–¢–ò–°–¢–ò–ö–£ (–¥–ª—è –º–∞–π–±—É—Ç–Ω—ñ—Ö –¥–æ—Å–ª—ñ–¥–Ω–∏–∫—ñ–≤ –±–æ–∂–µ–≤—ñ–ª–ª—è)
avg_freq_df.to_excel(f"{OUTPUT_DIR}/frequency_statistics.xlsx", index=False)
jaccard_df.to_excel(f"{OUTPUT_DIR}/jaccard_statistics.xlsx", index=False)

print("\n‚úÖ –ì–û–¢–û–í–û! –†–ï–ó–£–õ–¨–¢–ê–¢–ò –ó–ë–ï–†–ï–ñ–ï–ù–û –í –ü–ê–ü–¶–Ü 'results/':")
print("   - statistics.xlsx (–∑–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)")
print("   - phrases_by_type.xlsx (—Å–ª–æ–≤–Ω–∏–∫–∏)")
print("   - uniqueness_analysis.png (—É–Ω—ñ–∫–∞–ª—å–Ω—ñ—Å—Ç—å)")
print("   - top10_*.png (—Ç–æ–ø —Å–ª–æ–≤–æ—Å–ø–æ–ª—É—á–µ–Ω–Ω—è)")
print("   - pca_analysis.png (PCA)")
print("   - similarity_heatmap.png (—Å—Ö–æ–∂—ñ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä—ñ–≤)")
print("   - frequency_distribution.png (—Ä–æ–∑–ø–æ–¥—ñ–ª —á–∞—Å—Ç–æ—Ç)")
print("   - average_frequencies.png (—Å–µ—Ä–µ–¥–Ω—ñ —á–∞—Å—Ç–æ—Ç–∏)")
print("   - jaccard_analysis.png (—ñ–Ω–¥–µ–∫—Å –ñ–∞–∫–∫–∞—Ä–∞)")
print("   - frequency_statistics.xlsx (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞—Å—Ç–æ—Ç)")
print("   - jaccard_statistics.xlsx (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ö–æ–∂–æ—Å—Ç—ñ)")
print("\nüìä –†–ï–ó–Æ–ú–ï:")
print(f"   –í–°–¨–û–ì–û –ó–ù–ê–ô–î–ï–ù–û: {len(df)} —Å–ª–æ–≤–æ—Å–ø–æ–ª—É—á–µ–Ω—å (–≥–æ–ª–æ—Å—ñ–≤ –∑ —Ç–µ–º—Ä—è–≤–∏)")
print(f"   –£–ù–Ü–ö–ê–õ–¨–ù–ò–• –§–†–ê–ó: {len(freq_df)} (—ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω–∏—Ö –ø–æ—á–µ—Ä–∫—ñ–≤)")
print("\n   –Ü–ù–î–ï–ö–°–ò –ñ–ê–ö–ö–ê–†–ê (—Ä–æ–¥–∏–Ω–Ω—ñ—Å—Ç—å —Å–ª–æ–≤–Ω–∏–∫—ñ–≤):")
for _, row in jaccard_df.iterrows():
    type_name = {'verbal': '–î–Ü–Ñ–°–õ–Ü–í–ù–Ü', 'nominal': '–Ü–ú–ï–ù–ù–Ü', 'adverbial': '–ü–†–ò–°–õ–Ü–í–ù–ò–ö–û–í–Ü'}[row['type']]
    print(f"   {type_name}: {row['jaccard_index']:.3f} ({row['overlap_pct']:.1f}% –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è)")
